{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfe364-ac27-4c44-9af9-c448d4c84a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- Classification models\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier, RidgeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "# --- Regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor, BayesianRidge, LassoLars, PassiveAggressiveRegressor, SGDRegressor, RANSACRegressor, TheilSenRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Optional external libs (safe fallbacks if missing)\n",
    "have_xgb = have_lgbm = have_cat = True\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "except Exception:\n",
    "    have_xgb = False\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "except Exception:\n",
    "    have_lgbm = False\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "except Exception:\n",
    "    have_cat = False\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Step 1: Load dataset\n",
    "# ==========================\n",
    "def ask_path_and_target() -> Tuple[pd.DataFrame, str]:\n",
    "    print(\">>> Enter the path to your CSV dataset (e.g., C:/path/file.csv):\")\n",
    "    file_path = input().strip().strip('\"').strip(\"'\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"\\n‚úÖ Loaded dataset:\", file_path)\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "\n",
    "    print(\"\\n>>> Enter target (label) column name (leave blank to use LAST column):\")\n",
    "    tgt = input().strip()\n",
    "    if not tgt:\n",
    "        tgt = df.columns[-1]\n",
    "        print(f\"Using last column as target: '{tgt}'\")\n",
    "\n",
    "    return df, tgt\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Step 2: Clean & split\n",
    "# ==========================\n",
    "def basic_clean(df: pd.DataFrame, target: str, task_hint: str) -> pd.DataFrame:\n",
    "    \"\"\"Replace inf with NaN, tidy column names, and fill target NaNs.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Fill target NaNs (features are imputed in pipeline)\n",
    "    if target in df.columns:\n",
    "        missing = df[target].isnull().sum()\n",
    "        if missing > 0:\n",
    "            if task_hint == \"classification\":\n",
    "                fill_val = df[target].mode(dropna=True).iloc[0]\n",
    "            else:\n",
    "                fill_val = df[target].astype(float).mean()\n",
    "            df[target] = df[target].fillna(fill_val)\n",
    "            print(f\"‚ö†Ô∏è Filled {missing} missing target values with {fill_val}.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_X_y(df: pd.DataFrame, target: str):\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found.\")\n",
    "    y = df[target]\n",
    "    X = df.drop(columns=[target])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Step 3: Task detection (FIXED)\n",
    "# ==========================\n",
    "def _is_integer_like_series(y: pd.Series, tol=1e-9) -> bool:\n",
    "    \"\"\"True if dtype is integer or all non-null values are very close to integers.\"\"\"\n",
    "    if pd.api.types.is_integer_dtype(y):\n",
    "        return True\n",
    "    if pd.api.types.is_float_dtype(y):\n",
    "        s = y.dropna().values\n",
    "        if s.size == 0:\n",
    "            return False\n",
    "        return np.all(np.abs(s - np.round(s)) <= tol)\n",
    "    return False\n",
    "\n",
    "\n",
    "def problem_type(y: pd.Series) -> str:\n",
    "    # Explicit categorical types ‚Üí classification\n",
    "    if y.dtype == \"object\" or str(y.dtype).startswith(\"category\"):\n",
    "        return \"classification\"\n",
    "\n",
    "    # Count unique non-null values\n",
    "    unique = int(y.nunique(dropna=True))\n",
    "    n = int(len(y))\n",
    "\n",
    "    # Integers with small cardinality ‚Üí classification (labels like 0/1/2...)\n",
    "    if _is_integer_like_series(y) and (unique <= 20 or unique <= max(2, int(0.05 * n))):\n",
    "        return \"classification\"\n",
    "\n",
    "    # Otherwise ‚Üí regression (this fixes floats with few unique values being misclassified)\n",
    "    return \"regression\"\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Step 4: Preprocessor\n",
    "# ==========================\n",
    "def make_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    # NOTE: OneHotEncoder(sparse_output=False) requires sklearn >=1.2.\n",
    "    # If your env is older, change to OneHotEncoder(sparse=False).\n",
    "    cat_tf = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "    num_tf = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=True))\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_tf, num_cols),\n",
    "            (\"cat\", cat_tf, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Step 5: Model Zoos\n",
    "# ==========================\n",
    "def classification_models() -> Dict[str, object]:\n",
    "    models = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=5000),\n",
    "        \"LinearSVC\": LinearSVC(),\n",
    "        \"SVC_RBF\": SVC(kernel=\"rbf\"),\n",
    "        \"SVC_Poly\": SVC(kernel=\"poly\", degree=3),\n",
    "        \"KNN_Classifier\": KNeighborsClassifier(),\n",
    "        \"GaussianNB\": GaussianNB(),\n",
    "        \"MultinomialNB\": MultinomialNB(),\n",
    "        \"BernoulliNB\": BernoulliNB(),\n",
    "        \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
    "        \"RandomForestClassifier\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
    "        \"ExtraTreesClassifier\": ExtraTreesClassifier(n_estimators=300, random_state=42),\n",
    "        \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=42),\n",
    "        \"AdaBoostClassifier\": AdaBoostClassifier(random_state=42),\n",
    "        \"BaggingClassifier\": BaggingClassifier(random_state=42),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "        \"MLPClassifier\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300),\n",
    "        \"PassiveAggressiveClassifier\": PassiveAggressiveClassifier(random_state=42),\n",
    "        \"Perceptron\": Perceptron(random_state=42),\n",
    "        \"RidgeClassifier\": RidgeClassifier(),\n",
    "        \"SGDClassifier\": SGDClassifier(random_state=42),\n",
    "        \"NearestCentroid\": NearestCentroid(),\n",
    "    }\n",
    "    if have_xgb:\n",
    "        models[\"XGBClassifier\"] = XGBClassifier(\n",
    "            n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "            subsample=0.9, colsample_bytree=0.9,\n",
    "            random_state=42, n_jobs=-1, eval_metric=\"mlogloss\", tree_method=\"hist\"\n",
    "        )\n",
    "    if have_lgbm:\n",
    "        models[\"LGBMClassifier\"] = LGBMClassifier(\n",
    "            n_estimators=500, learning_rate=0.05, random_state=42\n",
    "        )\n",
    "    if have_cat:\n",
    "        models[\"CatBoostClassifier\"] = CatBoostClassifier(\n",
    "            iterations=500, depth=6, learning_rate=0.05, random_state=42, verbose=False\n",
    "        )\n",
    "    return models\n",
    "\n",
    "\n",
    "def regression_models() -> Dict[str, object]:\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(alpha=1.0),\n",
    "        \"Lasso\": Lasso(alpha=0.001, max_iter=10000),\n",
    "        \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000),\n",
    "        \"KernelRidge\": KernelRidge(kernel=\"rbf\"),\n",
    "        \"SVR_RBF\": SVR(kernel=\"rbf\"),\n",
    "        \"LinearSVR\": LinearSVR(),\n",
    "        \"KNN_Regressor\": KNeighborsRegressor(),\n",
    "        \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=42),\n",
    "        \"RandomForestRegressor\": RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "        \"ExtraTreesRegressor\": ExtraTreesRegressor(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "        \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=42),\n",
    "        \"AdaBoostRegressor\": AdaBoostRegressor(random_state=42),\n",
    "        \"HuberRegressor\": HuberRegressor(),\n",
    "        \"BayesianRidge\": BayesianRidge(),\n",
    "        \"LassoLars\": LassoLars(alpha=0.001),\n",
    "        \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(random_state=42),\n",
    "        \"SGDRegressor\": SGDRegressor(random_state=42, max_iter=2000),\n",
    "        \"RANSACRegressor\": RANSACRegressor(random_state=42),\n",
    "        \"TheilSenRegressor\": TheilSenRegressor(random_state=42),\n",
    "        \"MLPRegressor\": MLPRegressor(hidden_layer_sizes=(128, 64), max_iter=600),\n",
    "    }\n",
    "    if have_xgb:\n",
    "        models[\"XGBRegressor\"] = XGBRegressor(\n",
    "            n_estimators=400, max_depth=6, learning_rate=0.05,\n",
    "            subsample=0.9, colsample_bytree=0.9,\n",
    "            random_state=42, n_jobs=-1, tree_method=\"hist\"\n",
    "        )\n",
    "    if have_lgbm:\n",
    "        models[\"LGBMRegressor\"] = LGBMRegressor(\n",
    "            n_estimators=600, learning_rate=0.05, random_state=42\n",
    "        )\n",
    "    if have_cat:\n",
    "        models[\"CatBoostRegressor\"] = CatBoostRegressor(\n",
    "            iterations=600, depth=6, learning_rate=0.05, random_state=42, verbose=False\n",
    "        )\n",
    "    return models\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Step 6: Evaluation\n",
    "# ==========================\n",
    "def safe_cv(y: pd.Series, task: str):\n",
    "    if task == \"classification\":\n",
    "        min_class = int(pd.Series(y).value_counts().min())\n",
    "        n_splits = max(2, min(5, min_class))\n",
    "        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        return KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "def evaluate_models(X: pd.DataFrame, y: pd.Series, task: str, debug: bool = False):\n",
    "    pre = make_preprocessor(X)\n",
    "    cv = safe_cv(y, task)\n",
    "\n",
    "    results, failures = [], []\n",
    "    models = classification_models() if task == \"classification\" else regression_models()\n",
    "\n",
    "    # Detect negative features to safely skip MultinomialNB\n",
    "    try:\n",
    "        X_preview = pre.fit_transform(X.iloc[:100, :])\n",
    "        has_negative = np.nanmin(X_preview) < 0\n",
    "    except Exception:\n",
    "        has_negative = True\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if task == \"classification\" and name == \"MultinomialNB\" and has_negative:\n",
    "            failures.append((name, \"Skipped: requires non-negative features\"))\n",
    "            print(f\"{name:28s} -> Skipped: requires non-negative features\")\n",
    "            continue\n",
    "\n",
    "        pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "        scoring = \"f1_macro\" if task == \"classification\" else \"r2\"\n",
    "\n",
    "        try:\n",
    "            scores = cross_val_score(\n",
    "                pipe, X, y, cv=cv, scoring=scoring,\n",
    "                n_jobs=None, error_score=(\"raise\" if debug else np.nan)\n",
    "            )\n",
    "            mean_score = float(np.nanmean(scores))\n",
    "            results.append((name, mean_score))\n",
    "            print(f\"{name:28s} -> {scoring}: {mean_score:.4f}\")\n",
    "        except Exception as e:\n",
    "            failures.append((name, str(e)))\n",
    "            print(f\"{name:28s} -> ERROR: {e}\")\n",
    "\n",
    "    # Sort by score descending\n",
    "    results.sort(key=lambda x: (x[1] if x[1] is not None else -np.inf), reverse=True)\n",
    "    return results, failures\n",
    "\n",
    "\n",
    "def save_results_to_csv(results, failures, task: str, filename=\"model_results.csv\"):\n",
    "    metric = \"F1-macro\" if task == \"classification\" else \"R¬≤\"\n",
    "    rows = [{\"Model\": n, \"Score\": s, \"Metric\": metric, \"Status\": \"Success\"} for n, s in results]\n",
    "    rows += [{\"Model\": n, \"Score\": None, \"Metric\": metric, \"Status\": f\"Failed: {m}\"} for n, m in failures]\n",
    "    pd.DataFrame(rows).to_csv(filename, index=False)\n",
    "    print(f\"\\nüíæ Saved results to {filename}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Step 7: Main\n",
    "# ==========================\n",
    "def main():\n",
    "    df, target = ask_path_and_target()\n",
    "\n",
    "    # Initial guess from raw column\n",
    "    raw_task = problem_type(df[target])\n",
    "\n",
    "    # Clean with hint (so target imputation uses a sensible strategy)\n",
    "    df = basic_clean(df, target, raw_task)\n",
    "\n",
    "    # Final detection on cleaned target\n",
    "    X, y = split_X_y(df, target)\n",
    "    task = problem_type(y)\n",
    "\n",
    "    # Optional manual override\n",
    "    print(f\"\\nüîé Detected task: {task.upper()}\")\n",
    "    print(\"Press Enter to accept, or type 'classification' / 'regression' to override:\")\n",
    "    override = input().strip().lower()\n",
    "    if override in {\"classification\", \"regression\"}:\n",
    "        task = override\n",
    "        print(f\"‚û°Ô∏è  Overridden task: {task.upper()}\")\n",
    "\n",
    "    # Show quick target summary\n",
    "    if task == \"classification\":\n",
    "        vc = pd.Series(y).value_counts(dropna=False)\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(vc.to_string())\n",
    "    else:\n",
    "        print(f\"\\nTarget summary (numeric): count={y.shape[0]}, \"\n",
    "              f\"mean={pd.to_numeric(y, errors='coerce').mean():.4f}, \"\n",
    "              f\"std={pd.to_numeric(y, errors='coerce').std():.4f}\")\n",
    "\n",
    "    # Evaluate\n",
    "    results, failures = evaluate_models(X, y, task, debug=False)\n",
    "\n",
    "    if not results:\n",
    "        print(\"\\n‚ùå No model succeeded!\")\n",
    "        if failures:\n",
    "            print(\"Failures:\")\n",
    "            for n, m in failures:\n",
    "                print(f\"- {n}: {m}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    best_name, best_score = results[0]\n",
    "    metric = \"F1-macro\" if task == \"classification\" else \"R¬≤\"\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"üèÜ BEST MODEL\")\n",
    "    print(\"==============================\")\n",
    "    print(f\"Model : {best_name}\")\n",
    "    print(f\"Score : {best_score:.4f} ({metric} via cross-validation)\")\n",
    "\n",
    "    print(\"\\nüìä Top 5 models:\")\n",
    "    for i, (n, s) in enumerate(results[:5], 1):\n",
    "        print(f\"{i}. {n:28s} {s:.4f}\")\n",
    "\n",
    "    if failures:\n",
    "        print(\"\\n‚ÑπÔ∏è Models skipped/failed:\")\n",
    "        for name, msg in failures[:12]:\n",
    "            print(f\"- {name}: {msg}\")\n",
    "\n",
    "    # Save leaderboard\n",
    "    save_results_to_csv(results, failures, task)\n",
    "\n",
    "    # Train best model on full data & persist\n",
    "    zoo = classification_models() if task == \"classification\" else regression_models()\n",
    "    best_model = zoo[best_name]\n",
    "    final_pipe = Pipeline(steps=[(\"pre\", make_preprocessor(X)), (\"model\", best_model)])\n",
    "    final_pipe.fit(X, y)\n",
    "    joblib.dump(final_pipe, \"best_model.pkl\")\n",
    "    print(\"üíæ Saved best model to best_model.pkl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
